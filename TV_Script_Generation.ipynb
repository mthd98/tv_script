{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " TV Script Generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "pfkXyvzTC6WO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "import helper\n",
        "\n",
        "data_dir = './moes_tavern_lines.txt'\n",
        "text = helper.load_data(data_dir)\n",
        "# Ignore notice, since we don't use it for analysing the data\n",
        "text = text[81:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4Q54DMkER0C",
        "colab_type": "code",
        "outputId": "232712a2-480e-43df-b94f-a5b92dfc2141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "cell_type": "code",
      "source": [
        "view_sentence_range = (0, 10)\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "print('Dataset Stats')\n",
        "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
        "scenes = text.split('\\n\\n')\n",
        "print('Number of scenes: {}'.format(len(scenes)))\n",
        "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
        "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
        "\n",
        "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
        "print('Number of lines: {}'.format(len(sentences)))\n",
        "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
        "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
        "\n",
        "print()\n",
        "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
        "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Stats\n",
            "Roughly the number of unique words: 11492\n",
            "Number of scenes: 262\n",
            "Average number of sentences in each scene: 15.248091603053435\n",
            "Number of lines: 4257\n",
            "Average number of words in each line: 11.50434578341555\n",
            "\n",
            "The sentences 0 to 10:\n",
            "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
            "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
            "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
            "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
            "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
            "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
            "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
            "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wgWrrGSLFI-8",
        "colab_type": "code",
        "outputId": "5c3f270b-06d1-48ba-e8ab-ec1edf02326b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#lookup tables\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import problem_unittests as tests\n",
        "def create_lookup_tables(text):\n",
        "    \"\"\"\n",
        "    Create lookup tables for vocabulary\n",
        "    :param text: The text of tv scripts split into words\n",
        "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    vocab=set(text)\n",
        "\n",
        "    #words=text.split()\n",
        "    #counts=Counter(words)\n",
        "    #vocab=sorted(counts,key=counts.get,reverse=True)\n",
        "    vocab_to_int={word:ii for ii ,word in enumerate(vocab)}\n",
        "    int_to_vocab={ii:word for ii ,word in enumerate(vocab)}\n",
        "    return (vocab_to_int, int_to_vocab)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "tests.test_create_lookup_tables(create_lookup_tables)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HDvGtYhsLCbX",
        "colab_type": "code",
        "outputId": "fbc8ef5c-e0ef-4ef5-cd1d-a2cf9833a436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def token_lookup():\n",
        "    \"\"\"\n",
        "    Generate a dict to turn punctuation into a token.\n",
        "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    toke={'.':'||Period||',\n",
        "            ',':'||Comma||',\n",
        "            '\"':'||Quotation_Mark||',\n",
        "            ';':'||Semicolon||', \n",
        "            '!':'||Exclamation_Mark||',\n",
        "            '?':'||Question_Mark||', \n",
        "            '(':'||Left_Parentheses||', \n",
        "            ')':'||Right_Parentheses||',\n",
        "            '--':'||Dash||',\n",
        "            '\\n':'||Return||'}\n",
        "    return toke\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "tests.test_tokenize(token_lookup)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YvVKkozZTv_t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "# Preprocess Training, Validation, and Testing Data\n",
        "\n",
        "helper.preprocess_and_save_data(data_dir,token_lookup,create_lookup_tables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MRsiXPPaVfUj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "import helper\n",
        "import numpy as np\n",
        "import problem_unittests as tests\n",
        "\n",
        "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e1nQ7F4KV0oI",
        "colab_type": "code",
        "outputId": "411d1539-f9ef-4cac-9505-888cf2c84c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "from distutils.version import LooseVersion\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check TensorFlow Version\n",
        "assert LooseVersion(tf.__version__) >= LooseVersion('1.3'), 'Please use TensorFlow version 1.3 or newer'\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))\n",
        "\n",
        "# Check for a GPU\n",
        "if not tf.test.gpu_device_name():\n",
        "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
        "else:\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 1.13.1\n",
            "Default GPU Device: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XTFA0R0CV6At",
        "colab_type": "code",
        "outputId": "6af8b73b-e5dc-4343-af27-50e78918af30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def get_inputs():\n",
        "    \"\"\"\n",
        "    Create TF Placeholders for input, targets, and learning rate.\n",
        "    :return: Tuple (input, targets, learning rate)\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    input_text=tf.placeholder(tf.int32,shape=[None,None],name='input')\n",
        "    output_target=tf.placeholder(tf.int32,shape=[None,None],name='targets')\n",
        "    learning_rate=tf.placeholder(tf.float32,name='learning_rate')\n",
        "    \n",
        "    return (input_text, output_target, learning_rate)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "tests.test_get_inputs(get_inputs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QXqRzCkbX6Ls",
        "colab_type": "code",
        "outputId": "9152969c-416e-4e93-e15a-5420c8ed00af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def get_init_cell(batch_size, rnn_size):\n",
        "    \"\"\"\n",
        "    Create an RNN Cell and initialize it.\n",
        "    :param batch_size: Size of batches\n",
        "    :param rnn_size: Size of RNNs\n",
        "    :return: Tuple (cell, initialize state)\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    \n",
        "    def bulid_cell(rnn_size):\n",
        "      lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "      \n",
        "    # Add dropout to the cell outputs\n",
        "      #drop = tf.contrib.rnn.DropoutWrapper(lstm,keep_prob)\n",
        "      return lstm\n",
        "    # Stack up multiple LSTM layers, for deep learning\n",
        "    cell = tf.contrib.rnn.MultiRNNCell([bulid_cell(rnn_size) for _ in range(3)])\n",
        "    initial_state = cell.zero_state(batch_size,tf.float32)\n",
        "    \n",
        "    initial_state=tf.identity(initial_state,name='initial_state')\n",
        "    return (cell , initial_state)\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "tests.test_get_init_cell(get_init_cell)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hJlcnSDJgr1c",
        "colab_type": "code",
        "outputId": "9160723b-19c4-42a3-afa9-d0f31534c271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def get_embed(input_data, vocab_size, embed_dim):\n",
        "    \"\"\"\n",
        "    Create embedding for <input_data>.\n",
        "    :param input_data: TF placeholder for text input.\n",
        "    :param vocab_size: Number of words in vocabulary.\n",
        "    :param embed_dim: Number of embedding dimensions\n",
        "    :return: Embedded input.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    embedding=tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n",
        "    embed=tf.nn.embedding_lookup(embedding,input_data)\n",
        "    return embed\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "tests.test_get_embed(get_embed)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NcFDpUkjokcr",
        "colab_type": "code",
        "outputId": "d4d047cc-3b74-436f-f9b3-ea92c5f9db88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def build_rnn(cell, inputs):\n",
        "    \"\"\"\n",
        "    Create a RNN using a RNN Cell\n",
        "    :param cell: RNN Cell\n",
        "    :param inputs: Input text data\n",
        "    :return: Tuple (Outputs, Final State)\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    Outputs,final_state=tf.nn.dynamic_rnn(cell,inputs,dtype=tf.float32)\n",
        "    final_state=tf.identity(final_state,name='final_state')\n",
        "    return( Outputs, final_state)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "tests.test_build_rnn(build_rnn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AQ6FPMHIqEF_",
        "colab_type": "code",
        "outputId": "3cb4cd88-4d9e-4253-9b9f-486223d644d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
        "    \"\"\"\n",
        "    Build part of the neural network\n",
        "    :param cell: RNN cell\n",
        "    :param rnn_size: Size of rnns\n",
        "    :param input_data: Input data\n",
        "    :param vocab_size: Vocabulary size\n",
        "    :param embed_dim: Number of embedding dimensions\n",
        "    :return: Tuple (Logits, FinalState)\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    embedding_layer=get_embed(input_data,vocab_size,embed_dim)\n",
        "    outputs, final_state=build_rnn(cell,embedding_layer)\n",
        "    Logits=tf.layers.dense(outputs,vocab_size,activation=None,use_bias=True)\n",
        "    return (Logits, final_state)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "tests.test_build_nn(build_nn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X-g6CXKZr1zO",
        "colab_type": "code",
        "outputId": "d04be9e9-bf92-4052-e016-df856d088836",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def get_batches(int_text, batch_size, seq_length):\n",
        "    \"\"\"\n",
        "    Return batches of input and target\n",
        "    :param int_text: Text with the words replaced by their ids\n",
        "    :param batch_size: The size of batch\n",
        "    :param seq_length: The length of sequence\n",
        "    :return: Batches as a Numpy array\n",
        "    \"\"\"\n",
        "    # Calculate the number of batches\n",
        "    num_batches = len(int_text) // (batch_size  * seq_length)\n",
        "    # Drop long batches. Transform into a numpy array and reshape it for our purposes\n",
        "    np_text = np.array(int_text[:num_batches * (batch_size  * seq_length)])\n",
        "    # Reshape the data to give us the inputs sequence.\n",
        "    in_text = np_text.reshape(-1, seq_length)\n",
        "    # Roll (shift) and reshape to get target sequences (maybe not optimal)\n",
        "    tar_text = np.roll(np_text, -1).reshape(-1, seq_length)\n",
        "    output = np.zeros(shape=(num_batches, 2, batch_size, seq_length), dtype=np.int)\n",
        "    # Prepare the output\n",
        "    for idx in range(0, in_text.shape[0]):\n",
        "        jj = idx % num_batches\n",
        "        ii = idx // num_batches\n",
        "        output[jj,0,ii,:] = in_text[idx,:]\n",
        "        output[jj,1,ii,:] = tar_text[idx,:]\n",
        "    return output\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "tests.test_get_batches(get_batches)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sZs5k1bgtGio",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Number of Epochs\n",
        "num_epochs = 200\n",
        "# Batch Size\n",
        "batch_size = 128\n",
        "# RNN Size\n",
        "rnn_size = 256\n",
        "# Embedding Dimension Size\n",
        "embed_dim = 256\n",
        "# Sequence Length\n",
        "seq_length = 16\n",
        "# Learning Rate\n",
        "learning_rate = 0.001\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 100\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "save_dir = './save1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vyP7e33mtd6g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "from tensorflow.contrib import seq2seq\n",
        "\n",
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        "    vocab_size = len(int_to_vocab)\n",
        "    input_text, targets, lr = get_inputs()\n",
        "    input_data_shape = tf.shape(input_text)\n",
        "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
        "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
        "\n",
        "    # Probabilities for generating words\n",
        "    probs = tf.nn.softmax(logits, name='probs')\n",
        "\n",
        "    # Loss function\n",
        "    cost = seq2seq.sequence_loss(\n",
        "        logits,\n",
        "        targets,\n",
        "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "    # Gradient Clipping\n",
        "    gradients = optimizer.compute_gradients(cost)\n",
        "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
        "    train_op = optimizer.apply_gradients(capped_gradients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z3xjDhfztqHl",
        "colab_type": "code",
        "outputId": "01ec7d4a-4097-4f16-b392-6ec176d4bde4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "batches = get_batches(int_text, batch_size, seq_length)\n",
        "\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(num_epochs):\n",
        "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
        "\n",
        "        for batch_i, (x, y) in enumerate(batches):\n",
        "            feed = {\n",
        "                input_text: x,\n",
        "                targets: y,\n",
        "                initial_state: state,\n",
        "                lr: learning_rate}\n",
        "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
        "\n",
        "            # Show every <show_every_n_batches> batches\n",
        "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
        "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
        "                    epoch_i,\n",
        "                    batch_i,\n",
        "                    len(batches),\n",
        "                    train_loss))\n",
        "\n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, save_dir)\n",
        "    print('Model Trained and Saved')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Batch    0/33   train_loss = 8.821\n",
            "Epoch   3 Batch    1/33   train_loss = 5.920\n",
            "Epoch   6 Batch    2/33   train_loss = 5.891\n",
            "Epoch   9 Batch    3/33   train_loss = 5.943\n",
            "Epoch  12 Batch    4/33   train_loss = 5.926\n",
            "Epoch  15 Batch    5/33   train_loss = 6.064\n",
            "Epoch  18 Batch    6/33   train_loss = 5.950\n",
            "Epoch  21 Batch    7/33   train_loss = 5.977\n",
            "Epoch  24 Batch    8/33   train_loss = 5.802\n",
            "Epoch  27 Batch    9/33   train_loss = 5.420\n",
            "Epoch  30 Batch   10/33   train_loss = 5.155\n",
            "Epoch  33 Batch   11/33   train_loss = 4.677\n",
            "Epoch  36 Batch   12/33   train_loss = 4.661\n",
            "Epoch  39 Batch   13/33   train_loss = 4.491\n",
            "Epoch  42 Batch   14/33   train_loss = 4.367\n",
            "Epoch  45 Batch   15/33   train_loss = 4.161\n",
            "Epoch  48 Batch   16/33   train_loss = 4.075\n",
            "Epoch  51 Batch   17/33   train_loss = 3.990\n",
            "Epoch  54 Batch   18/33   train_loss = 3.885\n",
            "Epoch  57 Batch   19/33   train_loss = 3.667\n",
            "Epoch  60 Batch   20/33   train_loss = 3.642\n",
            "Epoch  63 Batch   21/33   train_loss = 3.596\n",
            "Epoch  66 Batch   22/33   train_loss = 3.517\n",
            "Epoch  69 Batch   23/33   train_loss = 3.340\n",
            "Epoch  72 Batch   24/33   train_loss = 3.269\n",
            "Epoch  75 Batch   25/33   train_loss = 3.164\n",
            "Epoch  78 Batch   26/33   train_loss = 3.000\n",
            "Epoch  81 Batch   27/33   train_loss = 2.940\n",
            "Epoch  84 Batch   28/33   train_loss = 2.827\n",
            "Epoch  87 Batch   29/33   train_loss = 2.770\n",
            "Epoch  90 Batch   30/33   train_loss = 2.775\n",
            "Epoch  93 Batch   31/33   train_loss = 2.574\n",
            "Epoch  96 Batch   32/33   train_loss = 2.519\n",
            "Epoch 100 Batch    0/33   train_loss = 2.445\n",
            "Epoch 103 Batch    1/33   train_loss = 2.316\n",
            "Epoch 106 Batch    2/33   train_loss = 2.249\n",
            "Epoch 109 Batch    3/33   train_loss = 2.202\n",
            "Epoch 112 Batch    4/33   train_loss = 2.161\n",
            "Epoch 115 Batch    5/33   train_loss = 1.987\n",
            "Epoch 118 Batch    6/33   train_loss = 1.889\n",
            "Epoch 121 Batch    7/33   train_loss = 1.855\n",
            "Epoch 124 Batch    8/33   train_loss = 1.798\n",
            "Epoch 127 Batch    9/33   train_loss = 1.741\n",
            "Epoch 130 Batch   10/33   train_loss = 1.630\n",
            "Epoch 133 Batch   11/33   train_loss = 1.660\n",
            "Epoch 136 Batch   12/33   train_loss = 1.574\n",
            "Epoch 139 Batch   13/33   train_loss = 1.525\n",
            "Epoch 142 Batch   14/33   train_loss = 1.399\n",
            "Epoch 145 Batch   15/33   train_loss = 1.430\n",
            "Epoch 148 Batch   16/33   train_loss = 1.380\n",
            "Epoch 151 Batch   17/33   train_loss = 1.380\n",
            "Epoch 154 Batch   18/33   train_loss = 1.271\n",
            "Epoch 157 Batch   19/33   train_loss = 1.221\n",
            "Epoch 160 Batch   20/33   train_loss = 1.178\n",
            "Epoch 163 Batch   21/33   train_loss = 1.067\n",
            "Epoch 166 Batch   22/33   train_loss = 1.038\n",
            "Epoch 169 Batch   23/33   train_loss = 1.031\n",
            "Epoch 172 Batch   24/33   train_loss = 0.995\n",
            "Epoch 175 Batch   25/33   train_loss = 0.936\n",
            "Epoch 178 Batch   26/33   train_loss = 0.914\n",
            "Epoch 181 Batch   27/33   train_loss = 0.871\n",
            "Epoch 184 Batch   28/33   train_loss = 0.822\n",
            "Epoch 187 Batch   29/33   train_loss = 0.807\n",
            "Epoch 190 Batch   30/33   train_loss = 0.762\n",
            "Epoch 193 Batch   31/33   train_loss = 0.732\n",
            "Epoch 196 Batch   32/33   train_loss = 0.741\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-VBMcS8v_PJE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "# Save parameters for checkpoint\n",
        "helper.save_params((seq_length, save_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcVY8NPI125A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import helper\n",
        "import problem_unittests as tests\n",
        "\n",
        "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "seq_length, load_dir = helper.load_params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IQLMCJDV16Tq",
        "colab_type": "code",
        "outputId": "1986c323-a95d-41d8-9ab5-468749717293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def get_tensors(loaded_graph):\n",
        "    \"\"\"\n",
        "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
        "    :param loaded_graph: TensorFlow graph loaded from file\n",
        "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    input_tensor = loaded_graph.get_tensor_by_name('input:0')\n",
        "    initial_state_tensor = loaded_graph.get_tensor_by_name('initial_state:0')\n",
        "    final_state_tensor = loaded_graph.get_tensor_by_name('final_state:0')\n",
        "    probs_tensor = loaded_graph.get_tensor_by_name('probs:0')\n",
        "    return (input_tensor, initial_state_tensor, final_state_tensor, probs_tensor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "tests.test_get_tensors(get_tensors)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F5D-vZKm2Dq7",
        "colab_type": "code",
        "outputId": "6e75fc47-4530-4ee2-da7a-01a669de6701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def pick_word(probabilities, int_to_vocab):\n",
        "    \"\"\"\n",
        "    Pick the next word in the generated text\n",
        "    :param probabilities: Probabilites of the next word\n",
        "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
        "    :return: String of the predicted word\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    argmax_pr = np.argmax(probabilities)\n",
        "    next_word = int_to_vocab[argmax_pr]\n",
        "    return next_word\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "tests.test_pick_word(pick_word)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "krq1RA562E1A",
        "colab_type": "code",
        "outputId": "7fa13b06-f91f-4daf-9500-ba1f5937c8a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "gen_length = 500\n",
        "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
        "prime_word = 'homer_simpson'\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
        "    loader.restore(sess, load_dir)\n",
        "\n",
        "    # Get Tensors from loaded model\n",
        "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
        "\n",
        "    # Sentences generation setup\n",
        "    gen_sentences = [prime_word + ':']\n",
        "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
        "\n",
        "    # Generate sentences\n",
        "    for n in range(gen_length):\n",
        "        # Dynamic Input\n",
        "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
        "        dyn_seq_length = len(dyn_input[0])\n",
        "\n",
        "        # Get Prediction\n",
        "        probabilities, prev_state = sess.run(\n",
        "            [probs, final_state],\n",
        "            {input_text: dyn_input, initial_state: prev_state})\n",
        "        \n",
        "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
        "\n",
        "        gen_sentences.append(pred_word)\n",
        "    \n",
        "    # Remove tokens\n",
        "    tv_script = ' '.join(gen_sentences)\n",
        "    for key, token in token_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
        "    tv_script = tv_script.replace('\\n ', '\\n')\n",
        "    tv_script = tv_script.replace('( ', '(')\n",
        "        \n",
        "    print(tv_script)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./save1\n",
            "homer_simpson:(loud) did you ever see that\" blue man group?\"\n",
            "duffman:(to moe) huh. i'm sweeter weird comin' and please.\n",
            "moe_szyslak: yeah, i didn't say that.\n",
            "moe_szyslak: oh what, that old beautiful, homer. i don't don't know what how it. i came to the springfield thing we got from the beach from the way!\n",
            "moe_szyslak:(waking-up voice.(then) oh my god, you're dead.\n",
            "moe_szyslak: what are you really are a little more of this best lady. i'm not just do the same, and we feel too to the drug customers.\n",
            "barney_gumble:(sighs) i can't drink i haven't seen that.\n",
            "moe_szyslak:(sincere) what's the genius, homer!\n",
            "homer_simpson:(toasting) you're almost out,\" dear midge:.\", i'll got away three says\" and then he been things as by her!\n",
            "lisa_simpson:(increasingly arm noises) we callin' a problem in our japanese stick too filthy has an ice kick this back.\n",
            "homer_simpson: well, i can't tell my english fat, i can spend the car again?\n",
            "lenny_leonard: and you know i-- who could make in our special card, lenny, it's true. i can't believe someone if you find some meet him.\n",
            "moe_szyslak: i'm laughing here. i don't do that?\n",
            "moe_szyslak: ah, no thing, but you're not all, uh, that's so bad. i can't he'll be someone at handle.\n",
            "moe_szyslak: ah, i'm looking to market you gotta care any more.\n",
            "homer_simpson:(to moe) yeah, it's sorry, this place that's too is all the drug broke, but i know, i made a love on the glass.\n",
            "moe_szyslak: ah, i'm going down to the dead tracks.(then chuckle) it's true before, moe. i want my best friend.\n",
            "moe_szyslak: oh of character, but i want to get us.\n",
            "moe_szyslak: well, i can't tell ya till this is okay. i don't be drinkin' with they wouldn't even drink you i'm be no.\n",
            "moe_szyslak: well, i am the\" few weeks.\"\n",
            "moe_szyslak:(laughs, moe) i guess we read it?\n",
            "moe_szyslak:(sad) i dunno, homer, this guy and i'm gonna made to see someone since their traditions in the world, but we go the glass of the young stuff you really i've love\"-- uh what only i got a hands off for we are you're an opening in your family.\n",
            "homer_simpson:(sighs) well, i guess what i think homer?\n",
            "moe_szyslak:(to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "afe1wEhF3qUh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}